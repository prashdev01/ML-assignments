{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability of an Employee Being a Smoker Given Health Insurance Usage:\n",
    "We can use Bayes’ theorem to find the probability that an employee is a smoker given that they use the health insurance plan.\n",
    "\n",
    "Let’s denote:\n",
    "* (S): Event that an employee is a smoker.\n",
    "* (H): Event that an employee uses the health insurance plan.\n",
    "\n",
    "We are given:\n",
    "* (P(H) = 0.7) (probability of using health insurance plan)\n",
    "* (P(S|H) = 0.4) (probability of being a smoker given health insurance usage)\n",
    "\n",
    "We want to find (P(S|H)).\n",
    "\n",
    "* Bayes’ theorem states: [ P(S|H) = \\frac{{P(H|S) \\cdot P(S)}}{{P(H)}} ]\n",
    "* Plugging in the given values: [ P(S|H) = \\frac{{0.4 \\cdot 0.7}}{{0.7}} = 0.4 ]\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that they use the health insurance plan is 0.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Both Bernoulli Naive Bayes (BNB) and Multinomial Naive Bayes (MNB) are variants of the Naive Bayes algorithm.\n",
    "* Bernoulli Naive Bayes:\n",
    "    * Used for binary features (e.g., presence/absence of a feature).\n",
    "    * Models the presence/absence of a feature.\n",
    "    * Counts how many times a feature does not occur.\n",
    "\n",
    "* Example: Text classification where each word is either present or absent.\n",
    "\n",
    "* Multinomial Naive Bayes:\n",
    "    * Widely used for document classification.\n",
    "    * Models the number of counts of a feature (e.g., word frequencies).\n",
    "    * Considers multiple features that occur.\n",
    "* Example: Classifying documents based on word frequencies.\n",
    "\n",
    "* In summary:\n",
    "BNB focuses on a single feature, while MNB considers multiple features and their counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q3. How does Bernoulli Naive Bayes handle missing values?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values in Bernoulli Naive Bayes:\n",
    "* Bernoulli Naive Bayes assumes binary features (presence/absence).\n",
    "* When dealing with missing values:\n",
    "    - Treat missing features as absent (i.e., value = 0).\n",
    "    - This aligns with the binary nature of BNB.\n",
    "    - It simplifies the model by not introducing additional complexity for handling missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q4. Can Gaussian Naive Bayes be used for multi-class classification?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes for Multi-Class Classification:\n",
    "* Gaussian Naive Bayes (GNB) assumes that features follow a Gaussian (normal) distribution.\n",
    "* GNB is primarily used for continuous-valued features.\n",
    "* It can be used for multi-class classification by extending the binary classification approach.\n",
    "* Each class has its own Gaussian distribution for each feature.\n",
    "* GNB calculates probabilities based on continuous feature values.\n",
    "* Therefore, yes, GNB can be used for multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q5. Assignment:**\n",
    "## **Data preparation:**\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "## **Implementation:**\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "# **Results:**\n",
    "**Report the following performance metrics for each classifier:**\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* F1 score\n",
    "# **Discussion:**\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "# **Conclusion:**\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: Bernoulli Naive Bayes\n",
      "Accuracy: 0.8839380364047911\n",
      "Precision: 0.8869617393737383\n",
      "Recall: 0.8152389047416673\n",
      "F1 Score: 0.8481249015095276\n",
      "\n",
      "Classifier: Multinomial Naive Bayes\n",
      "Accuracy: 0.7863496180326323\n",
      "Precision: 0.7393175533565436\n",
      "Recall: 0.7214983911116508\n",
      "F1 Score: 0.7282909724016348\n",
      "\n",
      "Classifier: Gaussian Naive Bayes\n",
      "Accuracy: 0.8217730830896915\n",
      "Precision: 0.7103733928118492\n",
      "Recall: 0.9569516119239877\n",
      "F1 Score: 0.8130660909542995\n",
      "\n",
      "Overall Summary:\n",
      "Classifier: Bernoulli Naive Bayes\n",
      "Accuracy: 0.8839380364047911\n",
      "Precision: 0.8869617393737383\n",
      "Recall: 0.8152389047416673\n",
      "F1 Score: 0.8481249015095276\n",
      "\n",
      "Classifier: Multinomial Naive Bayes\n",
      "Accuracy: 0.7863496180326323\n",
      "Precision: 0.7393175533565436\n",
      "Recall: 0.7214983911116508\n",
      "F1 Score: 0.7282909724016348\n",
      "\n",
      "Classifier: Gaussian Naive Bayes\n",
      "Accuracy: 0.8217730830896915\n",
      "Precision: 0.7103733928118492\n",
      "Recall: 0.9569516119239877\n",
      "F1 Score: 0.8130660909542995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "column_names = [\n",
    "    \"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\",\n",
    "    \"word_freq_our\", \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\",\n",
    "    \"word_freq_order\", \"word_freq_mail\", \"word_freq_receive\", \"word_freq_will\",\n",
    "    \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\", \"word_freq_free\",\n",
    "    \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\n",
    "    \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\",\n",
    "    \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\",\n",
    "    \"word_freq_lab\", \"word_freq_labs\", \"word_freq_telnet\", \"word_freq_857\",\n",
    "    \"word_freq_data\", \"word_freq_415\", \"word_freq_85\", \"word_freq_technology\",\n",
    "    \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\", \"word_freq_direct\",\n",
    "    \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\",\n",
    "    \"word_freq_re\", \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\",\n",
    "    \"char_freq_;\", \"char_freq_(\", \"char_freq_[\", \"char_freq_!\", \"char_freq_$\",\n",
    "    \"char_freq_#\", \"capital_run_length_average\", \"capital_run_length_longest\",\n",
    "    \"capital_run_length_total\", \"is_spam\"\n",
    "]\n",
    "data = pd.read_csv(url, names=column_names)\n",
    "\n",
    "# Split features and target variable\n",
    "X = data.drop('is_spam', axis=1)\n",
    "y = data['is_spam']\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"Bernoulli Naive Bayes\": BernoulliNB(),\n",
    "    \"Multinomial Naive Bayes\": MultinomialNB(),\n",
    "    \"Gaussian Naive Bayes\": GaussianNB()\n",
    "}\n",
    "\n",
    "# Initialize lists to store results\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "# Evaluate each classifier using cross-validation\n",
    "for name, classifier in classifiers.items():\n",
    "    # Perform 10-fold cross-validation\n",
    "    cv_accuracy = cross_val_score(classifier, X, y, cv=10, scoring='accuracy')\n",
    "    cv_precision = cross_val_score(classifier, X, y, cv=10, scoring='precision')\n",
    "    cv_recall = cross_val_score(classifier, X, y, cv=10, scoring='recall')\n",
    "    cv_f1 = cross_val_score(classifier, X, y, cv=10, scoring='f1')\n",
    "\n",
    "    # Calculate mean scores\n",
    "    accuracy_mean = cv_accuracy.mean()\n",
    "    precision_mean = cv_precision.mean()\n",
    "    recall_mean = cv_recall.mean()\n",
    "    f1_mean = cv_f1.mean()\n",
    "\n",
    "    # Append mean scores to lists\n",
    "    accuracy_scores.append(accuracy_mean)\n",
    "    precision_scores.append(precision_mean)\n",
    "    recall_scores.append(recall_mean)\n",
    "    f1_scores.append(f1_mean)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Classifier:\", name)\n",
    "    print(\"Accuracy:\", accuracy_mean)\n",
    "    print(\"Precision:\", precision_mean)\n",
    "    print(\"Recall:\", recall_mean)\n",
    "    print(\"F1 Score:\", f1_mean)\n",
    "    print()\n",
    "\n",
    "# Print overall summary\n",
    "print(\"Overall Summary:\")\n",
    "for name, accuracy, precision, recall, f1 in zip(classifiers.keys(), accuracy_scores, precision_scores, recall_scores, f1_scores):\n",
    "    print(\"Classifier:\", name)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
